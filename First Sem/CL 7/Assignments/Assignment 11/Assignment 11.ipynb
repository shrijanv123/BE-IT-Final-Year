{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Assignment 11.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7qzkcR3s1oH"
      },
      "source": [
        "# Neural Network using Cancer Datatset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJG84KiBs1oH"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "wAC5fROTs1oI",
        "outputId": "9568ce9d-e815-48ae-b5d7-057f47dddd4c"
      },
      "source": [
        "data = pd.read_csv('/content/sample_data/data.csv')\n",
        "data.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "      <th>Unnamed: 32</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842302</td>\n",
              "      <td>M</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>842517</td>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84300903</td>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84348301</td>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84358402</td>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id diagnosis  ...  fractal_dimension_worst  Unnamed: 32\n",
              "0    842302         M  ...                  0.11890          NaN\n",
              "1    842517         M  ...                  0.08902          NaN\n",
              "2  84300903         M  ...                  0.08758          NaN\n",
              "3  84348301         M  ...                  0.17300          NaN\n",
              "4  84358402         M  ...                  0.07678          NaN\n",
              "\n",
              "[5 rows x 33 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnSGpLqls1oI",
        "outputId": "4527f210-2703-4ba5-f205-01d6c5cf5260"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(569, 33)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czx0FLtts1oJ"
      },
      "source": [
        "### Preparing Dataset for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "WCJoYNe8s1oJ",
        "outputId": "101575db-a4ba-4aa6-a951-426ac0f44f92"
      },
      "source": [
        "cols = data.columns\n",
        "x_data = data[cols[2:-1]]\n",
        "x_data.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   radius_mean  texture_mean  ...  symmetry_worst  fractal_dimension_worst\n",
              "0        17.99         10.38  ...          0.4601                  0.11890\n",
              "1        20.57         17.77  ...          0.2750                  0.08902\n",
              "2        19.69         21.25  ...          0.3613                  0.08758\n",
              "3        11.42         20.38  ...          0.6638                  0.17300\n",
              "4        20.29         14.34  ...          0.2364                  0.07678\n",
              "\n",
              "[5 rows x 30 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Lu-2cy8s1oJ",
        "outputId": "ac3328ba-c0bf-48d7-ae09-b25512382dcf"
      },
      "source": [
        "y_data = data[cols[1]]\n",
        "le = LabelEncoder()\n",
        "y_data = np.array(le.fit_transform(y_data))\n",
        "print(y_data[:10])\n",
        "print(le.classes_)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 1 1 1 1 1 1 1 1]\n",
            "['B' 'M']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNParZhss1oJ"
      },
      "source": [
        "### Wrapping Dataset in type Variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mT_NCUyts1oJ"
      },
      "source": [
        "x_data = Variable(torch.from_numpy(x_data.values))\n",
        "y_data = Variable(torch.from_numpy(y_data))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zD3wl8JNs1oJ"
      },
      "source": [
        "### Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1dEJvGSs1oJ"
      },
      "source": [
        "class Model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.l1 = torch.nn.Linear(30,16)\n",
        "        self.l2 = torch.nn.Linear(16,4)\n",
        "        self.l3 = torch.nn.Linear(4,1)\n",
        "        \n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out1 = self.sigmoid(self.l1(x)) \n",
        "        out2 = self.sigmoid(self.l2(out1))\n",
        "        y_pred = self.sigmoid(self.l3(out2))\n",
        "        return y_pred\n",
        "    \n",
        "model = Model()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyzOThJKs1oJ"
      },
      "source": [
        "### Training of the Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAy2HQJvs1oK",
        "outputId": "c07cb208-9b90-465e-de13-5b493ec77991"
      },
      "source": [
        "criterion = torch.nn.BCELoss(reduction='sum')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epochs in range(400):\n",
        "    y_pred = model(x_data.float())\n",
        "    loss = criterion(y_pred, y_data.view(-1,1).float())\n",
        "    print('Epoch',epochs,'Loss:',loss.item(), '- Pred:', y_pred.data[0])\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 Loss: 377.9447021484375 - Pred: tensor([0.4017])\n",
            "Epoch 1 Loss: 377.6399230957031 - Pred: tensor([0.4006])\n",
            "Epoch 2 Loss: 377.3100891113281 - Pred: tensor([0.3995])\n",
            "Epoch 3 Loss: 376.8821105957031 - Pred: tensor([0.3985])\n",
            "Epoch 4 Loss: 376.5229187011719 - Pred: tensor([0.3974])\n",
            "Epoch 5 Loss: 376.3439025878906 - Pred: tensor([0.3964])\n",
            "Epoch 6 Loss: 376.28497314453125 - Pred: tensor([0.3953])\n",
            "Epoch 7 Loss: 376.25042724609375 - Pred: tensor([0.3943])\n",
            "Epoch 8 Loss: 376.2111511230469 - Pred: tensor([0.3933])\n",
            "Epoch 9 Loss: 376.1658630371094 - Pred: tensor([0.3922])\n",
            "Epoch 10 Loss: 376.12017822265625 - Pred: tensor([0.3912])\n",
            "Epoch 11 Loss: 376.0765686035156 - Pred: tensor([0.3902])\n",
            "Epoch 12 Loss: 376.03558349609375 - Pred: tensor([0.3892])\n",
            "Epoch 13 Loss: 375.9974670410156 - Pred: tensor([0.3882])\n",
            "Epoch 14 Loss: 375.96240234375 - Pred: tensor([0.3873])\n",
            "Epoch 15 Loss: 375.9313049316406 - Pred: tensor([0.3863])\n",
            "Epoch 16 Loss: 375.90521240234375 - Pred: tensor([0.3854])\n",
            "Epoch 17 Loss: 375.88134765625 - Pred: tensor([0.3844])\n",
            "Epoch 18 Loss: 375.8577880859375 - Pred: tensor([0.3835])\n",
            "Epoch 19 Loss: 375.8347473144531 - Pred: tensor([0.3827])\n",
            "Epoch 20 Loss: 375.81219482421875 - Pred: tensor([0.3818])\n",
            "Epoch 21 Loss: 375.7898864746094 - Pred: tensor([0.3810])\n",
            "Epoch 22 Loss: 375.7668762207031 - Pred: tensor([0.3802])\n",
            "Epoch 23 Loss: 375.7418518066406 - Pred: tensor([0.3794])\n",
            "Epoch 24 Loss: 375.7125549316406 - Pred: tensor([0.3786])\n",
            "Epoch 25 Loss: 375.6759033203125 - Pred: tensor([0.3779])\n",
            "Epoch 26 Loss: 375.6277160644531 - Pred: tensor([0.3772])\n",
            "Epoch 27 Loss: 375.5626220703125 - Pred: tensor([0.3766])\n",
            "Epoch 28 Loss: 375.47308349609375 - Pred: tensor([0.3760])\n",
            "Epoch 29 Loss: 375.3478698730469 - Pred: tensor([0.3754])\n",
            "Epoch 30 Loss: 375.1722412109375 - Pred: tensor([0.3749])\n",
            "Epoch 31 Loss: 374.9521484375 - Pred: tensor([0.3745])\n",
            "Epoch 32 Loss: 374.72418212890625 - Pred: tensor([0.3741])\n",
            "Epoch 33 Loss: 374.5556640625 - Pred: tensor([0.3737])\n",
            "Epoch 34 Loss: 374.5068359375 - Pred: tensor([0.3734])\n",
            "Epoch 35 Loss: 374.57659912109375 - Pred: tensor([0.3732])\n",
            "Epoch 36 Loss: 374.6517333984375 - Pred: tensor([0.3730])\n",
            "Epoch 37 Loss: 374.65631103515625 - Pred: tensor([0.3728])\n",
            "Epoch 38 Loss: 374.5840759277344 - Pred: tensor([0.3728])\n",
            "Epoch 39 Loss: 374.4636535644531 - Pred: tensor([0.3727])\n",
            "Epoch 40 Loss: 374.32733154296875 - Pred: tensor([0.3727])\n",
            "Epoch 41 Loss: 374.2068176269531 - Pred: tensor([0.3728])\n",
            "Epoch 42 Loss: 374.1275939941406 - Pred: tensor([0.3728])\n",
            "Epoch 43 Loss: 374.07720947265625 - Pred: tensor([0.3729])\n",
            "Epoch 44 Loss: 374.0232238769531 - Pred: tensor([0.3729])\n",
            "Epoch 45 Loss: 373.9609680175781 - Pred: tensor([0.3717])\n",
            "Epoch 46 Loss: 373.8668212890625 - Pred: tensor([0.3663])\n",
            "Epoch 47 Loss: 373.7208557128906 - Pred: tensor([0.3654])\n",
            "Epoch 48 Loss: 373.5071716308594 - Pred: tensor([0.3655])\n",
            "Epoch 49 Loss: 373.28656005859375 - Pred: tensor([0.3656])\n",
            "Epoch 50 Loss: 373.052490234375 - Pred: tensor([0.3658])\n",
            "Epoch 51 Loss: 372.89886474609375 - Pred: tensor([0.3659])\n",
            "Epoch 52 Loss: 372.77679443359375 - Pred: tensor([0.3661])\n",
            "Epoch 53 Loss: 372.74957275390625 - Pred: tensor([0.3664])\n",
            "Epoch 54 Loss: 372.6973876953125 - Pred: tensor([0.3666])\n",
            "Epoch 55 Loss: 372.5935974121094 - Pred: tensor([0.3669])\n",
            "Epoch 56 Loss: 372.4361877441406 - Pred: tensor([0.3672])\n",
            "Epoch 57 Loss: 372.2420654296875 - Pred: tensor([0.3675])\n",
            "Epoch 58 Loss: 372.04351806640625 - Pred: tensor([0.3678])\n",
            "Epoch 59 Loss: 371.9173583984375 - Pred: tensor([0.3682])\n",
            "Epoch 60 Loss: 371.81732177734375 - Pred: tensor([0.3685])\n",
            "Epoch 61 Loss: 371.7167663574219 - Pred: tensor([0.3688])\n",
            "Epoch 62 Loss: 371.6036071777344 - Pred: tensor([0.3691])\n",
            "Epoch 63 Loss: 371.463623046875 - Pred: tensor([0.3693])\n",
            "Epoch 64 Loss: 371.3048095703125 - Pred: tensor([0.3696])\n",
            "Epoch 65 Loss: 371.1429443359375 - Pred: tensor([0.3698])\n",
            "Epoch 66 Loss: 370.99261474609375 - Pred: tensor([0.3700])\n",
            "Epoch 67 Loss: 370.8580322265625 - Pred: tensor([0.3702])\n",
            "Epoch 68 Loss: 370.7295227050781 - Pred: tensor([0.3704])\n",
            "Epoch 69 Loss: 370.59552001953125 - Pred: tensor([0.3706])\n",
            "Epoch 70 Loss: 370.450927734375 - Pred: tensor([0.3708])\n",
            "Epoch 71 Loss: 370.2931823730469 - Pred: tensor([0.3709])\n",
            "Epoch 72 Loss: 370.1197204589844 - Pred: tensor([0.3711])\n",
            "Epoch 73 Loss: 369.9616394042969 - Pred: tensor([0.3712])\n",
            "Epoch 74 Loss: 369.831298828125 - Pred: tensor([0.3713])\n",
            "Epoch 75 Loss: 369.6956787109375 - Pred: tensor([0.3714])\n",
            "Epoch 76 Loss: 369.54766845703125 - Pred: tensor([0.3715])\n",
            "Epoch 77 Loss: 369.39306640625 - Pred: tensor([0.3716])\n",
            "Epoch 78 Loss: 369.2383117675781 - Pred: tensor([0.3716])\n",
            "Epoch 79 Loss: 369.088134765625 - Pred: tensor([0.3717])\n",
            "Epoch 80 Loss: 368.9429626464844 - Pred: tensor([0.3717])\n",
            "Epoch 81 Loss: 368.798583984375 - Pred: tensor([0.3717])\n",
            "Epoch 82 Loss: 368.64971923828125 - Pred: tensor([0.3717])\n",
            "Epoch 83 Loss: 368.4891357421875 - Pred: tensor([0.3717])\n",
            "Epoch 84 Loss: 368.3170471191406 - Pred: tensor([0.3718])\n",
            "Epoch 85 Loss: 368.1582946777344 - Pred: tensor([0.3720])\n",
            "Epoch 86 Loss: 368.00799560546875 - Pred: tensor([0.3729])\n",
            "Epoch 87 Loss: 367.8568420410156 - Pred: tensor([0.3748])\n",
            "Epoch 88 Loss: 367.6996765136719 - Pred: tensor([0.3772])\n",
            "Epoch 89 Loss: 367.5382080078125 - Pred: tensor([0.3789])\n",
            "Epoch 90 Loss: 367.3753662109375 - Pred: tensor([0.3799])\n",
            "Epoch 91 Loss: 367.2141418457031 - Pred: tensor([0.3804])\n",
            "Epoch 92 Loss: 367.05523681640625 - Pred: tensor([0.3809])\n",
            "Epoch 93 Loss: 366.89654541015625 - Pred: tensor([0.3819])\n",
            "Epoch 94 Loss: 366.73516845703125 - Pred: tensor([0.3832])\n",
            "Epoch 95 Loss: 366.57012939453125 - Pred: tensor([0.3842])\n",
            "Epoch 96 Loss: 366.404052734375 - Pred: tensor([0.3848])\n",
            "Epoch 97 Loss: 366.2420349121094 - Pred: tensor([0.3851])\n",
            "Epoch 98 Loss: 366.080322265625 - Pred: tensor([0.3852])\n",
            "Epoch 99 Loss: 365.91619873046875 - Pred: tensor([0.3854])\n",
            "Epoch 100 Loss: 365.7508850097656 - Pred: tensor([0.3855])\n",
            "Epoch 101 Loss: 365.5852355957031 - Pred: tensor([0.3856])\n",
            "Epoch 102 Loss: 365.419921875 - Pred: tensor([0.3857])\n",
            "Epoch 103 Loss: 365.25567626953125 - Pred: tensor([0.3858])\n",
            "Epoch 104 Loss: 365.0926513671875 - Pred: tensor([0.3859])\n",
            "Epoch 105 Loss: 364.9295959472656 - Pred: tensor([0.3860])\n",
            "Epoch 106 Loss: 364.765625 - Pred: tensor([0.3862])\n",
            "Epoch 107 Loss: 364.6007995605469 - Pred: tensor([0.3863])\n",
            "Epoch 108 Loss: 364.4358215332031 - Pred: tensor([0.3864])\n",
            "Epoch 109 Loss: 364.2706298828125 - Pred: tensor([0.3865])\n",
            "Epoch 110 Loss: 364.1041259765625 - Pred: tensor([0.3867])\n",
            "Epoch 111 Loss: 363.935546875 - Pred: tensor([0.3868])\n",
            "Epoch 112 Loss: 363.7646484375 - Pred: tensor([0.3869])\n",
            "Epoch 113 Loss: 363.5899353027344 - Pred: tensor([0.3871])\n",
            "Epoch 114 Loss: 363.4065856933594 - Pred: tensor([0.3872])\n",
            "Epoch 115 Loss: 363.21160888671875 - Pred: tensor([0.3874])\n",
            "Epoch 116 Loss: 363.01165771484375 - Pred: tensor([0.3875])\n",
            "Epoch 117 Loss: 362.8152770996094 - Pred: tensor([0.3877])\n",
            "Epoch 118 Loss: 362.6344299316406 - Pred: tensor([0.3878])\n",
            "Epoch 119 Loss: 362.47320556640625 - Pred: tensor([0.3880])\n",
            "Epoch 120 Loss: 362.3251647949219 - Pred: tensor([0.3881])\n",
            "Epoch 121 Loss: 362.15289306640625 - Pred: tensor([0.3882])\n",
            "Epoch 122 Loss: 361.95330810546875 - Pred: tensor([0.3884])\n",
            "Epoch 123 Loss: 361.7530212402344 - Pred: tensor([0.3885])\n",
            "Epoch 124 Loss: 361.5708923339844 - Pred: tensor([0.3886])\n",
            "Epoch 125 Loss: 361.3980712890625 - Pred: tensor([0.3888])\n",
            "Epoch 126 Loss: 361.22454833984375 - Pred: tensor([0.3889])\n",
            "Epoch 127 Loss: 361.046875 - Pred: tensor([0.3891])\n",
            "Epoch 128 Loss: 360.8632507324219 - Pred: tensor([0.3892])\n",
            "Epoch 129 Loss: 360.6733093261719 - Pred: tensor([0.3893])\n",
            "Epoch 130 Loss: 360.47711181640625 - Pred: tensor([0.3895])\n",
            "Epoch 131 Loss: 360.27716064453125 - Pred: tensor([0.3897])\n",
            "Epoch 132 Loss: 360.0835876464844 - Pred: tensor([0.3898])\n",
            "Epoch 133 Loss: 359.8990783691406 - Pred: tensor([0.3900])\n",
            "Epoch 134 Loss: 359.71337890625 - Pred: tensor([0.3901])\n",
            "Epoch 135 Loss: 359.5207824707031 - Pred: tensor([0.3903])\n",
            "Epoch 136 Loss: 359.322265625 - Pred: tensor([0.3904])\n",
            "Epoch 137 Loss: 359.12152099609375 - Pred: tensor([0.3906])\n",
            "Epoch 138 Loss: 358.92193603515625 - Pred: tensor([0.3907])\n",
            "Epoch 139 Loss: 358.7242736816406 - Pred: tensor([0.3909])\n",
            "Epoch 140 Loss: 358.52716064453125 - Pred: tensor([0.3910])\n",
            "Epoch 141 Loss: 358.3278503417969 - Pred: tensor([0.3912])\n",
            "Epoch 142 Loss: 358.1236877441406 - Pred: tensor([0.3913])\n",
            "Epoch 143 Loss: 357.9127502441406 - Pred: tensor([0.3915])\n",
            "Epoch 144 Loss: 357.6956787109375 - Pred: tensor([0.3917])\n",
            "Epoch 145 Loss: 357.47869873046875 - Pred: tensor([0.3918])\n",
            "Epoch 146 Loss: 357.2491760253906 - Pred: tensor([0.3920])\n",
            "Epoch 147 Loss: 356.95428466796875 - Pred: tensor([0.3921])\n",
            "Epoch 148 Loss: 356.6471862792969 - Pred: tensor([0.3923])\n",
            "Epoch 149 Loss: 356.4109802246094 - Pred: tensor([0.3924])\n",
            "Epoch 150 Loss: 356.1312255859375 - Pred: tensor([0.3925])\n",
            "Epoch 151 Loss: 355.78997802734375 - Pred: tensor([0.3925])\n",
            "Epoch 152 Loss: 355.3473815917969 - Pred: tensor([0.3925])\n",
            "Epoch 153 Loss: 354.79119873046875 - Pred: tensor([0.3925])\n",
            "Epoch 154 Loss: 354.0494384765625 - Pred: tensor([0.3925])\n",
            "Epoch 155 Loss: 353.0137939453125 - Pred: tensor([0.3924])\n",
            "Epoch 156 Loss: 351.6064147949219 - Pred: tensor([0.3923])\n",
            "Epoch 157 Loss: 350.04351806640625 - Pred: tensor([0.3922])\n",
            "Epoch 158 Loss: 348.72613525390625 - Pred: tensor([0.3922])\n",
            "Epoch 159 Loss: 347.984130859375 - Pred: tensor([0.3922])\n",
            "Epoch 160 Loss: 347.7572021484375 - Pred: tensor([0.3923])\n",
            "Epoch 161 Loss: 347.690185546875 - Pred: tensor([0.3925])\n",
            "Epoch 162 Loss: 347.39959716796875 - Pred: tensor([0.3928])\n",
            "Epoch 163 Loss: 346.8701171875 - Pred: tensor([0.3931])\n",
            "Epoch 164 Loss: 346.1873474121094 - Pred: tensor([0.3934])\n",
            "Epoch 165 Loss: 345.3490295410156 - Pred: tensor([0.3940])\n",
            "Epoch 166 Loss: 344.42462158203125 - Pred: tensor([0.3952])\n",
            "Epoch 167 Loss: 343.5674743652344 - Pred: tensor([0.4013])\n",
            "Epoch 168 Loss: 342.8076171875 - Pred: tensor([0.4104])\n",
            "Epoch 169 Loss: 342.3261413574219 - Pred: tensor([0.4133])\n",
            "Epoch 170 Loss: 341.99078369140625 - Pred: tensor([0.4143])\n",
            "Epoch 171 Loss: 341.58355712890625 - Pred: tensor([0.4149])\n",
            "Epoch 172 Loss: 341.09246826171875 - Pred: tensor([0.4154])\n",
            "Epoch 173 Loss: 340.5011901855469 - Pred: tensor([0.4162])\n",
            "Epoch 174 Loss: 339.79473876953125 - Pred: tensor([0.4172])\n",
            "Epoch 175 Loss: 339.06939697265625 - Pred: tensor([0.4180])\n",
            "Epoch 176 Loss: 338.4819030761719 - Pred: tensor([0.4186])\n",
            "Epoch 177 Loss: 338.0329895019531 - Pred: tensor([0.4192])\n",
            "Epoch 178 Loss: 337.6150207519531 - Pred: tensor([0.4197])\n",
            "Epoch 179 Loss: 337.1365661621094 - Pred: tensor([0.4203])\n",
            "Epoch 180 Loss: 336.5699157714844 - Pred: tensor([0.4208])\n",
            "Epoch 181 Loss: 335.9942932128906 - Pred: tensor([0.4213])\n",
            "Epoch 182 Loss: 335.4335632324219 - Pred: tensor([0.4218])\n",
            "Epoch 183 Loss: 334.833984375 - Pred: tensor([0.4223])\n",
            "Epoch 184 Loss: 334.2223205566406 - Pred: tensor([0.4229])\n",
            "Epoch 185 Loss: 333.70648193359375 - Pred: tensor([0.4234])\n",
            "Epoch 186 Loss: 333.2705078125 - Pred: tensor([0.4239])\n",
            "Epoch 187 Loss: 332.8009948730469 - Pred: tensor([0.4243])\n",
            "Epoch 188 Loss: 332.2572021484375 - Pred: tensor([0.4248])\n",
            "Epoch 189 Loss: 331.68701171875 - Pred: tensor([0.4252])\n",
            "Epoch 190 Loss: 331.14190673828125 - Pred: tensor([0.4257])\n",
            "Epoch 191 Loss: 330.6295166015625 - Pred: tensor([0.4261])\n",
            "Epoch 192 Loss: 330.1368408203125 - Pred: tensor([0.4266])\n",
            "Epoch 193 Loss: 329.65045166015625 - Pred: tensor([0.4271])\n",
            "Epoch 194 Loss: 329.15472412109375 - Pred: tensor([0.4276])\n",
            "Epoch 195 Loss: 328.63348388671875 - Pred: tensor([0.4282])\n",
            "Epoch 196 Loss: 328.09222412109375 - Pred: tensor([0.4287])\n",
            "Epoch 197 Loss: 327.5611572265625 - Pred: tensor([0.4293])\n",
            "Epoch 198 Loss: 327.05975341796875 - Pred: tensor([0.4298])\n",
            "Epoch 199 Loss: 326.5709533691406 - Pred: tensor([0.4304])\n",
            "Epoch 200 Loss: 326.06744384765625 - Pred: tensor([0.4310])\n",
            "Epoch 201 Loss: 325.5451354980469 - Pred: tensor([0.4316])\n",
            "Epoch 202 Loss: 325.0195617675781 - Pred: tensor([0.4322])\n",
            "Epoch 203 Loss: 324.50091552734375 - Pred: tensor([0.4328])\n",
            "Epoch 204 Loss: 323.9896240234375 - Pred: tensor([0.4334])\n",
            "Epoch 205 Loss: 323.48553466796875 - Pred: tensor([0.4340])\n",
            "Epoch 206 Loss: 322.9842529296875 - Pred: tensor([0.4346])\n",
            "Epoch 207 Loss: 322.47503662109375 - Pred: tensor([0.4353])\n",
            "Epoch 208 Loss: 321.9570617675781 - Pred: tensor([0.4359])\n",
            "Epoch 209 Loss: 321.4432678222656 - Pred: tensor([0.4366])\n",
            "Epoch 210 Loss: 320.9402770996094 - Pred: tensor([0.4372])\n",
            "Epoch 211 Loss: 320.44140625 - Pred: tensor([0.4379])\n",
            "Epoch 212 Loss: 319.9399719238281 - Pred: tensor([0.4385])\n",
            "Epoch 213 Loss: 319.43560791015625 - Pred: tensor([0.4392])\n",
            "Epoch 214 Loss: 318.929931640625 - Pred: tensor([0.4398])\n",
            "Epoch 215 Loss: 318.4245910644531 - Pred: tensor([0.4405])\n",
            "Epoch 216 Loss: 317.9223327636719 - Pred: tensor([0.4411])\n",
            "Epoch 217 Loss: 317.42327880859375 - Pred: tensor([0.4418])\n",
            "Epoch 218 Loss: 316.9234924316406 - Pred: tensor([0.4424])\n",
            "Epoch 219 Loss: 316.4197082519531 - Pred: tensor([0.4431])\n",
            "Epoch 220 Loss: 315.9134216308594 - Pred: tensor([0.4437])\n",
            "Epoch 221 Loss: 315.4082946777344 - Pred: tensor([0.4444])\n",
            "Epoch 222 Loss: 314.9052734375 - Pred: tensor([0.4450])\n",
            "Epoch 223 Loss: 314.40216064453125 - Pred: tensor([0.4457])\n",
            "Epoch 224 Loss: 313.8970947265625 - Pred: tensor([0.4463])\n",
            "Epoch 225 Loss: 313.39044189453125 - Pred: tensor([0.4470])\n",
            "Epoch 226 Loss: 312.8835754394531 - Pred: tensor([0.4476])\n",
            "Epoch 227 Loss: 312.3772888183594 - Pred: tensor([0.4482])\n",
            "Epoch 228 Loss: 311.87200927734375 - Pred: tensor([0.4488])\n",
            "Epoch 229 Loss: 311.367431640625 - Pred: tensor([0.4495])\n",
            "Epoch 230 Loss: 310.8628234863281 - Pred: tensor([0.4501])\n",
            "Epoch 231 Loss: 310.3577575683594 - Pred: tensor([0.4507])\n",
            "Epoch 232 Loss: 309.8532409667969 - Pred: tensor([0.4514])\n",
            "Epoch 233 Loss: 309.3503112792969 - Pred: tensor([0.4520])\n",
            "Epoch 234 Loss: 308.8486633300781 - Pred: tensor([0.4526])\n",
            "Epoch 235 Loss: 308.34747314453125 - Pred: tensor([0.4533])\n",
            "Epoch 236 Loss: 307.8465576171875 - Pred: tensor([0.4539])\n",
            "Epoch 237 Loss: 307.3462219238281 - Pred: tensor([0.4546])\n",
            "Epoch 238 Loss: 306.8466491699219 - Pred: tensor([0.4552])\n",
            "Epoch 239 Loss: 306.347900390625 - Pred: tensor([0.4559])\n",
            "Epoch 240 Loss: 305.8499755859375 - Pred: tensor([0.4565])\n",
            "Epoch 241 Loss: 305.35260009765625 - Pred: tensor([0.4572])\n",
            "Epoch 242 Loss: 304.8553466796875 - Pred: tensor([0.4579])\n",
            "Epoch 243 Loss: 304.3580322265625 - Pred: tensor([0.4586])\n",
            "Epoch 244 Loss: 303.86090087890625 - Pred: tensor([0.4593])\n",
            "Epoch 245 Loss: 303.364013671875 - Pred: tensor([0.4599])\n",
            "Epoch 246 Loss: 302.86737060546875 - Pred: tensor([0.4606])\n",
            "Epoch 247 Loss: 302.3710021972656 - Pred: tensor([0.4613])\n",
            "Epoch 248 Loss: 301.87469482421875 - Pred: tensor([0.4620])\n",
            "Epoch 249 Loss: 301.3780822753906 - Pred: tensor([0.4627])\n",
            "Epoch 250 Loss: 300.8817443847656 - Pred: tensor([0.4634])\n",
            "Epoch 251 Loss: 300.3865051269531 - Pred: tensor([0.4640])\n",
            "Epoch 252 Loss: 299.8923645019531 - Pred: tensor([0.4647])\n",
            "Epoch 253 Loss: 299.39886474609375 - Pred: tensor([0.4654])\n",
            "Epoch 254 Loss: 298.9061584472656 - Pred: tensor([0.4661])\n",
            "Epoch 255 Loss: 298.41473388671875 - Pred: tensor([0.4668])\n",
            "Epoch 256 Loss: 297.92449951171875 - Pred: tensor([0.4675])\n",
            "Epoch 257 Loss: 297.4350891113281 - Pred: tensor([0.4682])\n",
            "Epoch 258 Loss: 296.9465026855469 - Pred: tensor([0.4689])\n",
            "Epoch 259 Loss: 296.458984375 - Pred: tensor([0.4696])\n",
            "Epoch 260 Loss: 295.9723815917969 - Pred: tensor([0.4703])\n",
            "Epoch 261 Loss: 295.48651123046875 - Pred: tensor([0.4710])\n",
            "Epoch 262 Loss: 295.00115966796875 - Pred: tensor([0.4717])\n",
            "Epoch 263 Loss: 294.5165710449219 - Pred: tensor([0.4724])\n",
            "Epoch 264 Loss: 294.03271484375 - Pred: tensor([0.4731])\n",
            "Epoch 265 Loss: 293.5495300292969 - Pred: tensor([0.4738])\n",
            "Epoch 266 Loss: 293.0668029785156 - Pred: tensor([0.4745])\n",
            "Epoch 267 Loss: 292.584716796875 - Pred: tensor([0.4752])\n",
            "Epoch 268 Loss: 292.1032409667969 - Pred: tensor([0.4759])\n",
            "Epoch 269 Loss: 291.6223449707031 - Pred: tensor([0.4766])\n",
            "Epoch 270 Loss: 291.1420593261719 - Pred: tensor([0.4774])\n",
            "Epoch 271 Loss: 290.66229248046875 - Pred: tensor([0.4781])\n",
            "Epoch 272 Loss: 290.1831359863281 - Pred: tensor([0.4788])\n",
            "Epoch 273 Loss: 289.7045593261719 - Pred: tensor([0.4795])\n",
            "Epoch 274 Loss: 289.2266540527344 - Pred: tensor([0.4802])\n",
            "Epoch 275 Loss: 288.74932861328125 - Pred: tensor([0.4809])\n",
            "Epoch 276 Loss: 288.2726135253906 - Pred: tensor([0.4816])\n",
            "Epoch 277 Loss: 287.7965087890625 - Pred: tensor([0.4823])\n",
            "Epoch 278 Loss: 287.32110595703125 - Pred: tensor([0.4830])\n",
            "Epoch 279 Loss: 286.8463439941406 - Pred: tensor([0.4838])\n",
            "Epoch 280 Loss: 286.37225341796875 - Pred: tensor([0.4845])\n",
            "Epoch 281 Loss: 285.8987731933594 - Pred: tensor([0.4852])\n",
            "Epoch 282 Loss: 285.4259948730469 - Pred: tensor([0.4859])\n",
            "Epoch 283 Loss: 284.9539489746094 - Pred: tensor([0.4866])\n",
            "Epoch 284 Loss: 284.4825744628906 - Pred: tensor([0.4873])\n",
            "Epoch 285 Loss: 284.0118713378906 - Pred: tensor([0.4881])\n",
            "Epoch 286 Loss: 283.5419006347656 - Pred: tensor([0.4888])\n",
            "Epoch 287 Loss: 283.0726623535156 - Pred: tensor([0.4895])\n",
            "Epoch 288 Loss: 282.6041564941406 - Pred: tensor([0.4902])\n",
            "Epoch 289 Loss: 282.1363525390625 - Pred: tensor([0.4909])\n",
            "Epoch 290 Loss: 281.6692810058594 - Pred: tensor([0.4917])\n",
            "Epoch 291 Loss: 281.2029724121094 - Pred: tensor([0.4924])\n",
            "Epoch 292 Loss: 280.7374267578125 - Pred: tensor([0.4931])\n",
            "Epoch 293 Loss: 280.27264404296875 - Pred: tensor([0.4939])\n",
            "Epoch 294 Loss: 279.8086242675781 - Pred: tensor([0.4946])\n",
            "Epoch 295 Loss: 279.3453063964844 - Pred: tensor([0.4953])\n",
            "Epoch 296 Loss: 278.8828430175781 - Pred: tensor([0.4960])\n",
            "Epoch 297 Loss: 278.4211730957031 - Pred: tensor([0.4968])\n",
            "Epoch 298 Loss: 277.960205078125 - Pred: tensor([0.4975])\n",
            "Epoch 299 Loss: 277.50006103515625 - Pred: tensor([0.4982])\n",
            "Epoch 300 Loss: 277.04071044921875 - Pred: tensor([0.4990])\n",
            "Epoch 301 Loss: 276.5821533203125 - Pred: tensor([0.4997])\n",
            "Epoch 302 Loss: 276.1244201660156 - Pred: tensor([0.5004])\n",
            "Epoch 303 Loss: 275.66748046875 - Pred: tensor([0.5012])\n",
            "Epoch 304 Loss: 275.2113342285156 - Pred: tensor([0.5019])\n",
            "Epoch 305 Loss: 274.7560119628906 - Pred: tensor([0.5026])\n",
            "Epoch 306 Loss: 274.30145263671875 - Pred: tensor([0.5033])\n",
            "Epoch 307 Loss: 273.8477478027344 - Pred: tensor([0.5041])\n",
            "Epoch 308 Loss: 273.3948669433594 - Pred: tensor([0.5048])\n",
            "Epoch 309 Loss: 272.9427795410156 - Pred: tensor([0.5055])\n",
            "Epoch 310 Loss: 272.4915466308594 - Pred: tensor([0.5062])\n",
            "Epoch 311 Loss: 272.04107666015625 - Pred: tensor([0.5070])\n",
            "Epoch 312 Loss: 271.5914306640625 - Pred: tensor([0.5077])\n",
            "Epoch 313 Loss: 271.14263916015625 - Pred: tensor([0.5084])\n",
            "Epoch 314 Loss: 270.6946105957031 - Pred: tensor([0.5091])\n",
            "Epoch 315 Loss: 270.2474365234375 - Pred: tensor([0.5099])\n",
            "Epoch 316 Loss: 269.801025390625 - Pred: tensor([0.5106])\n",
            "Epoch 317 Loss: 269.35546875 - Pred: tensor([0.5113])\n",
            "Epoch 318 Loss: 268.9106750488281 - Pred: tensor([0.5120])\n",
            "Epoch 319 Loss: 268.4667053222656 - Pred: tensor([0.5128])\n",
            "Epoch 320 Loss: 268.0235290527344 - Pred: tensor([0.5135])\n",
            "Epoch 321 Loss: 267.5811462402344 - Pred: tensor([0.5142])\n",
            "Epoch 322 Loss: 267.1395263671875 - Pred: tensor([0.5150])\n",
            "Epoch 323 Loss: 266.69873046875 - Pred: tensor([0.5157])\n",
            "Epoch 324 Loss: 266.2586975097656 - Pred: tensor([0.5164])\n",
            "Epoch 325 Loss: 265.8194580078125 - Pred: tensor([0.5171])\n",
            "Epoch 326 Loss: 265.3809509277344 - Pred: tensor([0.5179])\n",
            "Epoch 327 Loss: 264.9432067871094 - Pred: tensor([0.5186])\n",
            "Epoch 328 Loss: 264.5061950683594 - Pred: tensor([0.5193])\n",
            "Epoch 329 Loss: 264.07000732421875 - Pred: tensor([0.5201])\n",
            "Epoch 330 Loss: 263.634521484375 - Pred: tensor([0.5208])\n",
            "Epoch 331 Loss: 263.1997985839844 - Pred: tensor([0.5215])\n",
            "Epoch 332 Loss: 262.76580810546875 - Pred: tensor([0.5222])\n",
            "Epoch 333 Loss: 262.3325500488281 - Pred: tensor([0.5230])\n",
            "Epoch 334 Loss: 261.9000244140625 - Pred: tensor([0.5237])\n",
            "Epoch 335 Loss: 261.46820068359375 - Pred: tensor([0.5244])\n",
            "Epoch 336 Loss: 261.037109375 - Pred: tensor([0.5252])\n",
            "Epoch 337 Loss: 260.6067810058594 - Pred: tensor([0.5259])\n",
            "Epoch 338 Loss: 260.1771240234375 - Pred: tensor([0.5266])\n",
            "Epoch 339 Loss: 259.7481689453125 - Pred: tensor([0.5273])\n",
            "Epoch 340 Loss: 259.3199768066406 - Pred: tensor([0.5281])\n",
            "Epoch 341 Loss: 258.8924255371094 - Pred: tensor([0.5288])\n",
            "Epoch 342 Loss: 258.4656066894531 - Pred: tensor([0.5295])\n",
            "Epoch 343 Loss: 258.0395202636719 - Pred: tensor([0.5303])\n",
            "Epoch 344 Loss: 257.61407470703125 - Pred: tensor([0.5310])\n",
            "Epoch 345 Loss: 257.1893615722656 - Pred: tensor([0.5317])\n",
            "Epoch 346 Loss: 256.76531982421875 - Pred: tensor([0.5324])\n",
            "Epoch 347 Loss: 256.34197998046875 - Pred: tensor([0.5332])\n",
            "Epoch 348 Loss: 255.91934204101562 - Pred: tensor([0.5339])\n",
            "Epoch 349 Loss: 255.49737548828125 - Pred: tensor([0.5346])\n",
            "Epoch 350 Loss: 255.07608032226562 - Pred: tensor([0.5353])\n",
            "Epoch 351 Loss: 254.65548706054688 - Pred: tensor([0.5361])\n",
            "Epoch 352 Loss: 254.2355499267578 - Pred: tensor([0.5368])\n",
            "Epoch 353 Loss: 253.81631469726562 - Pred: tensor([0.5375])\n",
            "Epoch 354 Loss: 253.39772033691406 - Pred: tensor([0.5382])\n",
            "Epoch 355 Loss: 252.97979736328125 - Pred: tensor([0.5390])\n",
            "Epoch 356 Loss: 252.56259155273438 - Pred: tensor([0.5397])\n",
            "Epoch 357 Loss: 252.14596557617188 - Pred: tensor([0.5404])\n",
            "Epoch 358 Loss: 251.73004150390625 - Pred: tensor([0.5411])\n",
            "Epoch 359 Loss: 251.31475830078125 - Pred: tensor([0.5419])\n",
            "Epoch 360 Loss: 250.900146484375 - Pred: tensor([0.5426])\n",
            "Epoch 361 Loss: 250.48617553710938 - Pred: tensor([0.5433])\n",
            "Epoch 362 Loss: 250.07284545898438 - Pred: tensor([0.5440])\n",
            "Epoch 363 Loss: 249.66018676757812 - Pred: tensor([0.5448])\n",
            "Epoch 364 Loss: 249.2481689453125 - Pred: tensor([0.5455])\n",
            "Epoch 365 Loss: 248.83680725097656 - Pred: tensor([0.5462])\n",
            "Epoch 366 Loss: 248.42605590820312 - Pred: tensor([0.5469])\n",
            "Epoch 367 Loss: 248.01596069335938 - Pred: tensor([0.5476])\n",
            "Epoch 368 Loss: 247.6065216064453 - Pred: tensor([0.5483])\n",
            "Epoch 369 Loss: 247.19776916503906 - Pred: tensor([0.5491])\n",
            "Epoch 370 Loss: 246.7896728515625 - Pred: tensor([0.5498])\n",
            "Epoch 371 Loss: 246.3822479248047 - Pred: tensor([0.5505])\n",
            "Epoch 372 Loss: 245.9755096435547 - Pred: tensor([0.5512])\n",
            "Epoch 373 Loss: 245.56944274902344 - Pred: tensor([0.5519])\n",
            "Epoch 374 Loss: 245.1641082763672 - Pred: tensor([0.5526])\n",
            "Epoch 375 Loss: 244.75946044921875 - Pred: tensor([0.5534])\n",
            "Epoch 376 Loss: 244.35549926757812 - Pred: tensor([0.5541])\n",
            "Epoch 377 Loss: 243.95230102539062 - Pred: tensor([0.5548])\n",
            "Epoch 378 Loss: 243.5498504638672 - Pred: tensor([0.5555])\n",
            "Epoch 379 Loss: 243.1481475830078 - Pred: tensor([0.5562])\n",
            "Epoch 380 Loss: 242.74716186523438 - Pred: tensor([0.5569])\n",
            "Epoch 381 Loss: 242.34693908691406 - Pred: tensor([0.5577])\n",
            "Epoch 382 Loss: 241.94741821289062 - Pred: tensor([0.5584])\n",
            "Epoch 383 Loss: 241.54861450195312 - Pred: tensor([0.5591])\n",
            "Epoch 384 Loss: 241.15045166015625 - Pred: tensor([0.5598])\n",
            "Epoch 385 Loss: 240.75289916992188 - Pred: tensor([0.5605])\n",
            "Epoch 386 Loss: 240.35585021972656 - Pred: tensor([0.5612])\n",
            "Epoch 387 Loss: 239.9590301513672 - Pred: tensor([0.5620])\n",
            "Epoch 388 Loss: 239.56202697753906 - Pred: tensor([0.5627])\n",
            "Epoch 389 Loss: 239.16390991210938 - Pred: tensor([0.5634])\n",
            "Epoch 390 Loss: 238.76194763183594 - Pred: tensor([0.5641])\n",
            "Epoch 391 Loss: 238.34841918945312 - Pred: tensor([0.5648])\n",
            "Epoch 392 Loss: 237.92459106445312 - Pred: tensor([0.5655])\n",
            "Epoch 393 Loss: 237.5162353515625 - Pred: tensor([0.5661])\n",
            "Epoch 394 Loss: 237.09396362304688 - Pred: tensor([0.5668])\n",
            "Epoch 395 Loss: 236.7000732421875 - Pred: tensor([0.5674])\n",
            "Epoch 396 Loss: 236.30870056152344 - Pred: tensor([0.5681])\n",
            "Epoch 397 Loss: 235.91246032714844 - Pred: tensor([0.5688])\n",
            "Epoch 398 Loss: 235.5281982421875 - Pred: tensor([0.5694])\n",
            "Epoch 399 Loss: 235.14527893066406 - Pred: tensor([0.5701])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNknJ1nis1oK",
        "outputId": "a4a9466e-540c-4799-c9b7-36610388f3e4"
      },
      "source": [
        "x_data.data[0]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.7990e+01, 1.0380e+01, 1.2280e+02, 1.0010e+03, 1.1840e-01, 2.7760e-01,\n",
              "        3.0010e-01, 1.4710e-01, 2.4190e-01, 7.8710e-02, 1.0950e+00, 9.0530e-01,\n",
              "        8.5890e+00, 1.5340e+02, 6.3990e-03, 4.9040e-02, 5.3730e-02, 1.5870e-02,\n",
              "        3.0030e-02, 6.1930e-03, 2.5380e+01, 1.7330e+01, 1.8460e+02, 2.0190e+03,\n",
              "        1.6220e-01, 6.6560e-01, 7.1190e-01, 2.6540e-01, 4.6010e-01, 1.1890e-01],\n",
              "       dtype=torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xauOnTzps1oK",
        "outputId": "6cb79441-f4e6-4777-b84d-96e4d796ef06"
      },
      "source": [
        "y_pred.data.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([569, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yw7jtQ0Ys1oK"
      },
      "source": [
        "pred=model.double().forward(x_data) > 0.5"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kAMRttbs1oK",
        "outputId": "9f9b4893-e8dc-4518-e334-afed49d77a2a"
      },
      "source": [
        "pred.numpy()[:5]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [ True]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6S77kguJs1oL",
        "outputId": "f90f6d52-6be7-4ebc-9ec6-30cf534f732d"
      },
      "source": [
        "a = pred.numpy()\n",
        "b = y_data.numpy()\n",
        "pred.numpy().reshape(-1).shape, y_data.numpy().shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((569,), (569,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tI72qq0s1oL"
      },
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yA-RGouMs1oL",
        "outputId": "7c1a9fe3-ba35-464b-f765-be3c5c455beb"
      },
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "c = confusion_matrix(a,b)\n",
        "print(c)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[352  35]\n",
            " [  5 177]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHzyMQA9s1oL",
        "outputId": "0f38be33-e000-413b-d05f-4ff710fd88b8"
      },
      "source": [
        "print(classification_report(a,b))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.99      0.91      0.95       387\n",
            "        True       0.83      0.97      0.90       182\n",
            "\n",
            "    accuracy                           0.93       569\n",
            "   macro avg       0.91      0.94      0.92       569\n",
            "weighted avg       0.94      0.93      0.93       569\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lezKNnAfs1oL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}